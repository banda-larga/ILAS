{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fanpage.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udx8zn-_SYWC"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"40%\" src=\"https://upload.wikimedia.org/wikipedia/commons/1/16/Fanpage.it.png\">\n",
        "</p>\n",
        "Dataset per Abstract Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2XPAwyfSyRK"
      },
      "source": [
        "import requests\n",
        "\n",
        "# pagina\n",
        "r = requests.get(\"https://www.fanpage.it/politica/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3PcxaXLVqjH"
      },
      "source": [
        "!pip install -q colorama"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kfO02eoVjeX"
      },
      "source": [
        "import requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import colorama\n",
        "\n",
        " colori\n",
        "colorama.init()\n",
        "GREEN = colorama.Fore.GREEN\n",
        "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
        "RESET = colorama.Fore.RESET\n",
        "YELLOW = colorama.Fore.YELLOW\n",
        "\n",
        "internal_urls = set()\n",
        "external_urls = set()\n",
        "\n",
        "def is_valid(url):\n",
        "    \"\"\"\n",
        "    Controlla che l'url sia valido.\n",
        "    \"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-bifrOGUvBL"
      },
      "source": [
        "def get_all_website_links(url):\n",
        "    \"\"\"\n",
        "    Tutti gli urls che vengono trovati nell'url che appartiene al sito\n",
        "    \"\"\"\n",
        "    # tutti gli URLs `url`\n",
        "    urls = set()\n",
        "    # dominio senza prot\n",
        "    domain_name = urlparse(url).netloc\n",
        "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
        "    for a_tag in soup.findAll(\"a\"):\n",
        "      href = a_tag.attrs.get(\"href\")\n",
        "      if href == \"\" or href is None:\n",
        "          # href tag vuota\n",
        "          continue\n",
        "      href = urljoin(url, href)\n",
        "      parsed_href = urlparse(href)\n",
        "      # rimuovi parametri URL GET, URL fragments, etc.\n",
        "      href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
        "\n",
        "      if not is_valid(href):\n",
        "          # URL non valido\n",
        "          continue\n",
        "      if href in internal_urls:\n",
        "          # giÃ  visto\n",
        "          continue\n",
        "      if domain_name not in href:\n",
        "          # link estrerno\n",
        "          #if href not in external_urls:\n",
        "              #external_urls.add(href)\n",
        "          continue\n",
        "      urls.add(href)\n",
        "      internal_urls.add(href)\n",
        "    return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSH4xtojVFbO"
      },
      "source": [
        "# numero di url visitati\n",
        "total_urls_visited = 0\n",
        "\n",
        "def crawl(url, max_urls=50):\n",
        "    \"\"\"\n",
        "    Crawlerrr\n",
        "    `external_urls` e `internal_urls` global set variables\n",
        "    \"\"\"\n",
        "    global total_urls_visited\n",
        "    total_urls_visited += 1\n",
        "    if (total_urls_visited%500 == 0):\n",
        "      print(\"Total URLs visited: \", total_urls_visited)\n",
        "      print(f\"{YELLOW}[*] Now Crawling: {url}{RESET}\")\n",
        "    \n",
        "    links = get_all_website_links(url)\n",
        "    links = [link for link in links if link in internal_urls]\n",
        "    for link in links:\n",
        "        if total_urls_visited > 3000:\n",
        "            break\n",
        "        try:\n",
        "          crawl(link, max_urls=max_urls)\n",
        "        except: \n",
        "          pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxJZUgwyqqJB"
      },
      "source": [
        "import sys\n",
        "sys.setrecursionlimit(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHL8o2SjVHwy"
      },
      "source": [
        "# setting internal_urls-external_urls empty\n",
        "internal_urls = set()\n",
        "external_urls = set()\n",
        "\n",
        "crawl(\"https://www.fanpage.it/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7wY1EuuZ39V"
      },
      "source": [
        "len(internal_urls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNbnJGdjaRFH"
      },
      "source": [
        "# save text file with urls\n",
        "textfile = open(\"fanpage_story.txt\", \"w\")\n",
        "\n",
        "for element in internal_urls:\n",
        "    textfile.write(element + \"\\n\")\n",
        "textfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM9GPnotpzvW"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNI_exjLPgp7"
      },
      "source": [
        "!pip install -q --upgrade spacy\n",
        "!pip install -q rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxeM0KffCqo9"
      },
      "source": [
        "import spacy.cli\n",
        "import string\n",
        "\n",
        "spacy.cli.download(\"it_core_news_sm\")\n",
        "nlp = spacy.load(\"it_core_news_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGlakRlJUkMo"
      },
      "source": [
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# split summary into sentences and compute rougeL with article text\n",
        "def rouge_l(titolo, testo): \n",
        "  testo = testo.translate(str.maketrans('', '', string.punctuation))\n",
        "  doc = nlp(titolo)\n",
        "  for sents in doc.sents:\n",
        "    sents = str(sents)\n",
        "    sents = sents.translate(str.maketrans('', '', string.punctuation))\n",
        "    if scorer.score(testo, sents).get('rougeL')[0] >= 0.95:\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7iqxZiD1-j7"
      },
      "source": [
        "from tqdm import tqdm\n",
        "titles = []\n",
        "articles = []\n",
        "\n",
        "# get abstract and article from fanpage urls\n",
        "num_lines = sum(1 for line in open(\"fanpage_complete.txt\",'r'))\n",
        "with open(\"fanpage_complete.txt\",'r') as f:\n",
        "    for line in tqdm(f, total=num_lines):\n",
        "      try:\n",
        "        url = str(line).rstrip()\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')        \n",
        "        titolo_articolo = soup.find(\"div\", {\"class\":\"fp_intro__abstract\"}).get_text()\n",
        "        try:\n",
        "          testo_articolo = soup.find_all(\"p\")[1:10]\n",
        "        except:\n",
        "          testo_articolo = \"\"\n",
        "          \n",
        "        paragraphtext = []\n",
        "        for paragraph in testo_articolo[:-1]:\n",
        "            # testo\n",
        "            text = paragraph.get_text()\n",
        "            paragraphtext.append(text)\n",
        "\n",
        "        testo_fin = ' '.join(paragraphtext) \n",
        "        # controllo intersezione e che il titono non sia nell'articolo\n",
        "        if rouge_l(titolo_articolo,testo_fin):\n",
        "          titles.append(titolo_articolo)\n",
        "          articles.append(testo_fin)\n",
        "      except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNyasqY7vg_9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# some cleaning removing empty rows \n",
        "df = pd.DataFrame({\"source\":articles, \"target\":titles})\n",
        "df.head()\n",
        "df.shape[0]\n",
        "\n",
        "df['source'].replace(\"\", np.nan, inplace=True)\n",
        "df['target'].replace(\"\", np.nan, inplace=True)\n",
        "df.dropna(subset=['source'], inplace=True)\n",
        "df.dropna(subset=['target'], inplace=True)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(df.source.size, df.target.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea6taAsXwM62"
      },
      "source": [
        "df.to_csv(\"/content/Fanpage_design.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmPJjrdiMhck"
      },
      "source": [
        "### plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1NLLCRzwbAP"
      },
      "source": [
        "import re\n",
        "word_bound = re.compile(r'\\b')\n",
        "\n",
        "def num_words(line):\n",
        "    return len(word_bound.findall(line)) >> 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C5O4a0BwccS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') \n",
        "from matplotlib.colors import to_rgba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9p3QX4oweBj"
      },
      "source": [
        "ii = [i for i in range(0,80)]\n",
        "lunghezze_riass = [num_words(df[\"target\"].iloc[i]) for i in range(df.shape[0])]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "\n",
        "fig.suptitle('lunghezze riassunti', fontsize=14, fontweight='bold')\n",
        "ax.set_title('Fanpage')\n",
        "\n",
        "ax.set_xlabel('tokens')\n",
        "ax.set_ylabel('numero')\n",
        "\n",
        "#plt.style.use(['dark_background'])\n",
        "plt.hist(lunghezze_riass, bins=ii, color='c')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMbYKB1dwiuY"
      },
      "source": [
        "ii = [i for i in range(0,500,6)]\n",
        "lunghezze_docu = [num_words(df['source'].iloc[i]) for i in range(df.shape[0])]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "\n",
        "fig.suptitle('lunghezze documenti', fontsize=14, fontweight='bold')\n",
        "ax.set_title('Fanpage')\n",
        "\n",
        "ax.set_xlabel('tokens')\n",
        "ax.set_ylabel('numero')\n",
        "\n",
        "#plt.style.use(['dark_background'])\n",
        "plt.hist(lunghezze_docu, bins=ii, color='c')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38MMk02fw2ZG"
      },
      "source": [
        "# text cleaning\n",
        "\n",
        "We used textacy for text cleaning, normalizing unicode, removing html tags and normalizing whitespaces "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDfoUNGAw4Qg"
      },
      "source": [
        "!pip install -q textacy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iNUqylE0dcW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read dataset csv\n",
        "df = pd.read_csv(\"/content/Fanpage.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS5ICSEhw74H"
      },
      "source": [
        "from textacy import preprocessing\n",
        "clean_inputs = []\n",
        "clean_targets = []\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "     preprocessing.normalize.unicode,\n",
        "     preprocessing.remove.html_tags,\n",
        "     preprocessing.normalize.whitespace\n",
        ")\n",
        "\n",
        "# apply to dataset elements\n",
        "clean_inputs = [preproc(df[\"source\"].iloc[i]) for i in range(df.shape[0])]\n",
        "clean_targets = [preproc(df[\"target\"].iloc[i]) for i in range(df.shape[0])]\n",
        "df = pd.DataFrame({\"source\": clean_inputs, \"target\": clean_targets})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaYn1cn0xJj1"
      },
      "source": [
        "df.to_csv(\"/content/Fanpage_clean.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsDyJPXFKta2"
      },
      "source": [
        "# sets for filtering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uORgIkOXKunL"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "def get_lines(file):\n",
        "  return open(file, 'r').readlines()\n",
        "\n",
        "mypath = \"/content/total/\"\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "\n",
        "total = 0\n",
        "filter = set()\n",
        "for file in onlyfiles:\n",
        "  lines = get_lines(mypath + file)\n",
        "  total += len(lines)\n",
        "  filter.update(lines)\n",
        "\n",
        "print(total, len(filter))\n",
        "\n",
        "lines = [line.rstrip() for line in filter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdWXlYPDLw_O"
      },
      "source": [
        "textfile = open(\"fanpage_complete.txt\", \"w\")\n",
        "\n",
        "for element in lines:\n",
        "    textfile.write(element + \"\\n\")\n",
        "textfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}