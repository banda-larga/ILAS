{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IlPost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nwxVGb1MR4Y"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <img width=\"20%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Il_Post_logo.svg/800px-Il_Post_logo.svg.png\">\n",
        "</p>\n",
        "Dataset per Abstract Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2XNqL1eaNFc"
      },
      "source": [
        "import requests\n",
        "\n",
        "# pagina\n",
        "r = requests.get(\"https://www.ilpost.it/economia\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVcS6Zg8ePzz"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(r.text, \"html5lib\")\n",
        "articles = soup.find_all(\"article\")\n",
        "\n",
        "print(BeautifulSoup(str(articles[6]), 'html.parser').prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KejTKBWHeypz"
      },
      "source": [
        "summary = []\n",
        "for i in range(len(articles)):\n",
        "    try: \n",
        "      testo1 = articles[i].select_one(\"h1 a, h2 a, h3 a\").get_text().strip()\n",
        "    except:\n",
        "      \"\"\n",
        "    try:\n",
        "      testo2 = articles[i].select_one(\"p\").get_text().strip()\n",
        "      testo2 = testo2[0].lower() + testo2[1:]\n",
        "    except:\n",
        "      \"\"\n",
        "    riassunto = testo1 + \" \" + testo2\n",
        "    print(str(i) + \": \" + riassunto)\n",
        "    summary.append((riassunto))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjPPZ3UMllgw"
      },
      "source": [
        "pagelinks = []\n",
        "for link in articles:\n",
        "  url = link.find_all('a')[0]\n",
        "  pagelinks.append(url.get('href'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f5lLZqinNbn"
      },
      "source": [
        "print(len(articles), len(pagelinks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQcGHmn3mR1s"
      },
      "source": [
        "thearticle = []\n",
        "paragraph_test = []\n",
        "for link in pagelinks:    \n",
        "    paragraphtext = []    \n",
        "    url = link\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')        \n",
        "\n",
        "    try:\n",
        "      testo_articolo = soup.find_all(\"p\")[1:4]\n",
        "    except:\n",
        "      testo_articolo = \"\"\n",
        "    for paragraph in testo_articolo[:-1]:\n",
        "        # get the text only\n",
        "        text = paragraph.get_text()\n",
        "        paragraphtext.append(text)\n",
        "\n",
        "    testo_fin = ' '.join(paragraphtext)     \n",
        "    thearticle.append(testo_fin)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz7WI5h7AqmL"
      },
      "source": [
        "print(\"Titolo: \\n\" + summary[4], \"\\n\\nArticolo: \\n\" + thearticle[4] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR18KxcXFY8I"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABfcPScdFhpa"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "det_e65WGO9d"
      },
      "source": [
        "Num. of pages for every topic (from website)\n",
        "\n",
        "\n",
        "\n",
        "*   Tecnologia: 97\n",
        "*   Politica: 133\n",
        "*   Italia: 414\n",
        "*   Cultura: 593\n",
        "*   Economia: 148\n",
        "*   Scienza: 148\n",
        "*   Sport: 381\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TybVAGHH48K"
      },
      "source": [
        "argo = [\"economia\", \"tecnologia\", \"politica\", \"italia\", \"cultura\", \"scienza\", \"sport\", \"internet\"]\n",
        "popol = [148, 97, 184, 414, 594, 149, 381, 114]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRbi2nPRuwAv"
      },
      "source": [
        "#@title ## Topic\n",
        "#@markdown Choose.\n",
        "\n",
        "#@markdown ---\n",
        "argomento = \"italia\" #@param [\"economia\", \"tecnologia\", \"politica\",\"italia\",\"cultura\",\"scienza\",\"sport\",\"internet\"]\n",
        "argomenti = [argomento]\n",
        "pop = [popol[argo.index(argomento)]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gO23oOCJAVy"
      },
      "source": [
        "import time\n",
        "\n",
        "def coppie(url):\n",
        "  time.sleep(1)\n",
        "  r = requests.get(url)\n",
        "  soup = BeautifulSoup(r.text, \"html5lib\")\n",
        "  articles = soup.find_all(\"article\")\n",
        "  \n",
        "  # list of summaries\n",
        "  summary = []\n",
        "  for i in range(len(articles)):\n",
        "    try: \n",
        "      testo1 = articles[i].select_one(\"h1 a, h2 a, h3 a\").get_text().strip()\n",
        "    except:\n",
        "      \"\"\n",
        "    try:\n",
        "      testo2 = articles[i].select_one(\"p\").get_text().strip()\n",
        "    except:\n",
        "      \"\"\n",
        "\n",
        "    if testo1[-1] == \"?\" or testo1[-1] == \".\": \n",
        "      if testo2 == \"\": riassunto = testo1\n",
        "      elif testo2[-1] == \".\": riassunto = testo1 + \" \" + testo2\n",
        "      else: riassunto = testo1 + \" \" + testo2[0].lower() + testo2[1:] + \".\"\n",
        "    elif testo1[-1] == \",\":\n",
        "      if testo2 == \"\": riassunto = testo1[:-1] + \".\"\n",
        "      elif testo2[-1] == \".\": riassunto = testo1 + \" \" + testo2[0].lower() + testo2[1:]\n",
        "      else: riassunto = testo1 + \" \" + testo2[0].lower() + testo2[1:] + \".\"\n",
        "    elif testo1[-1] != \",\": \n",
        "      if testo2 == \"\": riassunto = testo1 + \".\"\n",
        "      elif testo2[-1] == \".\": riassunto = testo1 + \". \" + testo2\n",
        "      else: riassunto = testo1 + \". \" + testo2 + \".\"\n",
        "    summary.append((riassunto))\n",
        "\n",
        "  # articles urls\n",
        "  pagelinks = []\n",
        "  for link in articles:\n",
        "    url = link.find_all('a')[0]\n",
        "    pagelinks.append(url.get('href'))\n",
        "\n",
        "  thearticle = []\n",
        "  paragraph_test = []\n",
        "  for link in pagelinks:    \n",
        "    paragraphtext = []    \n",
        "    url = link\n",
        "    page = requests.get(url)\n",
        "    time.sleep(0.1)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')        \n",
        "\n",
        "    try:\n",
        "      testo_articolo = soup.find_all(\"p\")[1:4]\n",
        "    except:\n",
        "      testo_articolo = \"\"\n",
        "    for paragraph in testo_articolo[:-1]:\n",
        "        text = paragraph.get_text()\n",
        "        paragraphtext.append(text)\n",
        "\n",
        "    testo_fin = ' '.join(paragraphtext)     \n",
        "    thearticle.append(testo_fin)\n",
        "  d = {'source': thearticle, 'target': summary}\n",
        "  return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkbdSZ3kGSNX"
      },
      "source": [
        "# get topic summaries and articles\n",
        "df = pd.DataFrame(columns=[\"source\", \"target\"])\n",
        "i = 0\n",
        "j = 0\n",
        "conto = 0\n",
        "for i in range(len(argomenti)):\n",
        "  for j in range(1, pop[i]):\n",
        "    if j == 1:      \n",
        "      ret = coppie(\"https://www.ilpost.it/\" + argomenti[i]+\"/\")\n",
        "      conto += 26\n",
        "      print(\"\\nPagina: \" + \"https://www.ilpost.it/\" + argomenti[i]+\"/\"\n",
        "      + \"\\nArticoli Scaricati: \" + str(conto))\n",
        "      df = pd.concat([df, pd.DataFrame(ret)])\n",
        "    else: \n",
        "      ret = coppie(\"https://www.ilpost.it/\" + argomenti[i] + \"/page/\" + str(j) +\"/\")\n",
        "      conto += 26\n",
        "      print(\"\\nPagina: \" + \"https://www.ilpost.it/\" + argomenti[i] + \"/page/\" + str(j) +\"/\"\n",
        "      + \"\\nArticoli Scaricati: \" + str(conto))\n",
        "      #ret = coppie(\"https://www.ilpost.it/%s/page/%s/\"%(argomenti[i], str(j)))\n",
        "      df = pd.concat([df, pd.DataFrame(ret)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e24vk61HgQZB"
      },
      "source": [
        "df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRh3hkUNy6_W"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# some cleaning, remove empty rows \n",
        "df['source'].replace(\"\", np.nan, inplace=True)\n",
        "df['target'].replace(\"\", np.nan, inplace=True)\n",
        "df.dropna(subset=['source'], inplace=True)\n",
        "df.dropna(subset=['target'], inplace=True)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(df.source.size, df.target.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrhZHCyzkd3n"
      },
      "source": [
        "df.to_csv(\"/content/IlPost_Italia.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmhMWrszNUlZ"
      },
      "source": [
        "### plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFgHaFxUeddP"
      },
      "source": [
        "import re\n",
        "word_bound = re.compile(r'\\b')\n",
        "\n",
        "def num_words(line):\n",
        "    return len(word_bound.findall(line)) >> 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2xt4dpoegPM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') \n",
        "from matplotlib.colors import to_rgba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFJUxIq_ei2N"
      },
      "source": [
        "ii = [i for i in range(0,80)]\n",
        "lunghezze_riass = [num_words(df[\"target\"].iloc[i]) for i in range(df.shape[0])]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "\n",
        "fig.suptitle('lunghezze riassunti', fontsize=14, fontweight='bold')\n",
        "ax.set_title('IlPost')\n",
        "\n",
        "ax.set_xlabel('tokens')\n",
        "ax.set_ylabel('numero')\n",
        "\n",
        "#plt.style.use(['dark_background'])\n",
        "plt.hist(lunghezze_riass, bins=ii, color='c')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1EbCRjsfLSr"
      },
      "source": [
        "ii = [i for i in range(0,500,6)]\n",
        "lunghezze_docu = [num_words(df['source'].iloc[i]) for i in range(df.shape[0])]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "\n",
        "fig.suptitle('lunghezze documenti', fontsize=14, fontweight='bold')\n",
        "ax.set_title('IlPost')\n",
        "\n",
        "ax.set_xlabel('tokens')\n",
        "ax.set_ylabel('numero')\n",
        "\n",
        "#plt.style.use(['dark_background'])\n",
        "plt.hist(lunghezze_docu, bins=ii, color='c')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SilkJ3yHM9YS"
      },
      "source": [
        "# Text cleaning\n",
        "\n",
        "We used textacy for text cleaning, normalizing unicode, removing html tags and normalizing whitespaces "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op0Mb8A-NDHq"
      },
      "source": [
        "!pip install -q textacy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWzJpe_aO5KD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read dataset csv \n",
        "df = pd.read_csv(\"IlPost_Tech.csv\")\n",
        "df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE53ugDwNptm"
      },
      "source": [
        "from textacy import preprocessing\n",
        "clean_inputs = []\n",
        "clean_targets = []\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "     preprocessing.normalize.unicode,\n",
        "     preprocessing.remove.html_tags,\n",
        "     preprocessing.normalize.whitespace\n",
        ")\n",
        "\n",
        "# apply to dataset elements\n",
        "clean_inputs = [preproc(df[\"source\"].iloc[i]) for i in range(df.shape[0])]\n",
        "clean_targets = [preproc(df[\"target\"].iloc[i]) for i in range(df.shape[0])]\n",
        "df = pd.DataFrame({\"source\": clean_inputs, \"target\": clean_targets})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U89_EuA_QrW3"
      },
      "source": [
        "df.to_csv(\"/content/IlPost_Italia.csv\", sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
